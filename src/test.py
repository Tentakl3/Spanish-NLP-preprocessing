from nltk import word_tokenize

text = ['I prepared a sandwich for breakfast.',
        'We prepared hamburgers for lunch.',
        'I bought a book on cooking.',
        'You will buy a politics magazine.'
        ]

print(f'\nOriginal text: \n\n{text}')

#tokenize sentences
text_tokenized = [word_tokenize(sent) for sent in text]
print(f'\nText tokenized: \n\n{text_tokenized}')


text_lower = [[word.lower() for word in sent] for sent in text_tokenized]
print(f'\nText tokenized and lower-cased: \n\n{text_lower}')

#make vocabulary
text_flattened = [word for sent in text_lower for word in sent] 
print(f'\nText flattened: \n\n{text_flattened}')

vocabulary = sorted(set(text_flattened))
print(f'\nVocabulary: \n\n{vocabulary}')
print(f'\nVocabulary has {len(vocabulary)} words\n')


#collect contexts for each word of the vocabulary
window=8
contexts = {}
for word in vocabulary:
    context = []
    for i in range(len(text_flattened)):
        if text_flattened[i] == word:
            for j in range(i-int(window/2), i): #left context
                if j >= 0: 
                    context.append(text_flattened[j])
            try:
                for j in range(i+1, i+(int(window/2)+1)): #right context
                    context.append(text_flattened[j])
            except IndexError:
                pass
    contexts[word] = context

#print term-document matrix
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

contexts_list = contexts.values()
print(f'\nList of contexts: \n\n{contexts_list}')
print(f'\nThere are {len(contexts_list)} contexts')

#docs = ['why hello there', 'omg hello pony', 'she went there? omg']
contexts_strings = [' '.join(x) for x in contexts_list]
#print(contexts_strings)
print(f'\nThere are {len(contexts_strings)} contexts strings')


vec = CountVectorizer(token_pattern='(?u)\\b\\w+\\b')
X = vec.fit_transform(contexts_strings)
df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())
print(f'\nTerm-document matrix:\n {df}')
#print(df)

vocab = vec.get_feature_names_out()
print(vocab)
print(f'\nVocabulary generated by CountVectorizer:\n {vocab}')
print(f'\nVocabulary generated by CountVectorizer has {len(vocab)} words')

