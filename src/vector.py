#print term-document matrix
import pickle as pk
import pandas as pd
#from sklearn.feature_extraction.text import CountVectorizer


class Vector:
    """Class to represent the vectorization of text"""
    def __init__(self, tokens, text, vocabulary):
        self.tokens = tokens
        self.text = text
        self.vocabulary = vocabulary

    def check_context(self):
        contextfile = open('contextPickle', 'rb')    
        contextdb = pk.load(contextfile)
        for keys in contextdb:
            print(keys, '=>', contextdb[keys])
        contextfile.close()

    def context(self):
        """collect contexts for each word of the vocabulary"""
        window=8
        contexts = {}
        for word in self.vocabulary:
            context = []
            for i in range(len(self.text)):
                if self.text[i] == word:
                    for j in range(i-int(window/2), i): #left context
                        if j >= 0: 
                            context.append(self.text[j])
                    try:
                        for j in range(i+1, i+(int(window/2)+1)): #right context
                            context.append(self.text[j])
                    except IndexError:
                        pass
            contexts[word] = context

        contextfile = open('contextPickle', 'ab')
        pk.dump(contexts, contextfile)
        contextfile.close()

"""
#docs = ['why hello there', 'omg hello pony', 'she went there? omg']
contexts_strings = [' '.join(x) for x in contexts_list]
#print(contexts_strings)
print(f'\nThere are {len(contexts_strings)} contexts strings')


vec = CountVectorizer()
X = vec.fit_transform(contexts_strings)
df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())
print(f'\nTerm-document matrix:\n {df}')
#print(df)

vocab = vec.get_feature_names_out()
print(vocab)
print(f'\nVocabulary generated by CountVectorizer:\n {vocab}')
print(f'\nVocabulary generated by CountVectorizer has {len(vocab)} words')
"""